To use the BigQuery API we need API credentials. Following are the steps to retrieve API credentials:

First step is to create a project in google cloud and enable a service account following this link https://cloud.google.com/iam/docs/service-account-overview
For this, first navigate to Dashboard of Google cloud and then follow link IAM&Admin > Service Account.
Create a service account and download the JSON file and replace the contents of JSON file in the file researchproposal.json

Next step is to enable BigQuery API.
For this, navigate to Dashboard > APIs& Services > Library
Search for BigQuery API and enable the API.
Now we are ready to start working with BigQuery API.

The folder contains 5 files:
Prepare_dataset.py 
Prepare_Normal_Dataset.py
Merge_files.py
Merge_Norm_files.py
Clean_DS.ipynb

Instructions for running code:

1) Prepare_dataset.py 
 Prepares dataset of sentences containing code words, takes file Code_word_file.txt as input and generates two output files, CodeWord_Sentences.txt, Log_Main1.txt.
 Modify the local path to file researchproposal.json file in the code
 
2) Prepare_Normal_Dataset.py
Prepares dataset of sentences containing normal words, takes file Normal_Word_file.txt as input and generates two output files. Normal_sentences1.txt, Log_Normal_sentences1.txt
Modify the local path to file researchproposal.json file in the code

3) Merge files.py and 4) Merge_Norm_files.py
Replace the path to the 'BigQuery\Files' folder in the code. 
These two python files merge all sentences in files that are generated by running Prepare_dataset.py  and Prepare_Normal_Dataset.py.
It produces two output files, CodeWord_DS.txt, NormWord_DS.txt

5) Clean_DS.ipynb
Cleanes all data and produces final combined Dataset which is used in further experiments, i.e. CombinesDS_2.txt.

Note: File Final_DS_testing.txt was created manually by adding labels to file generated after step4 (CodeWord_DS.txt, NormWord_DS.txt)
Actual output files created during my experiments are placed in the "Files" folder